#!/usr/bin/env python3
"""
å®Œæ•´çš„MoSLoRAæµ‹è¯•ï¼šGPUè®­ç»ƒ + PEFTé€‚é…å™¨åŠ è½½
"""

import os
import sys
import subprocess
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# MoSLoRA integration: ensure the local customized PEFT is importable
_BASE_DIR = os.path.dirname(os.path.abspath(__file__))
_LOCAL_PEFT_PATH = os.path.join(_BASE_DIR, "MosLora", "peft", "src")
if os.path.isdir(_LOCAL_PEFT_PATH) and _LOCAL_PEFT_PATH not in sys.path:
    sys.path.append(_LOCAL_PEFT_PATH)

try:
    from peft import PeftModel
    print("âœ“ Successfully imported custom PEFT library")
except Exception as e:
    print(f"âœ— Failed to import custom PEFT library: {e}")
    sys.exit(1)

def test_gpu_training():
    """æµ‹è¯•GPUä¸Šçš„MoSLoRAè®­ç»ƒ"""
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âœ— CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒæµ‹è¯•")
        return False
    
    print(f"âœ“ æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUè®¾å¤‡")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    
    # è®­ç»ƒå‚æ•°
    base_model_path = "/home/hangshuaihe/tlm/tlm_dataset/gen/gen_data/clm_gen_best_v100_v4"
    train_file = "/home/hangshuaihe/tlm/tlm_dataset/gen/gen_data/sft_dataset_v100_v5/0_merge.json"
    output_dir = "/home/hangshuaihe/tlm/gen/test_gpu_moslora_output"
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = [
        "python", "train_clm.py",
        "--model_name_or_path", base_model_path,
        "--train_file", train_file,
        "--output_dir", output_dir,
        "--use_moslora",  # å¯ç”¨MoSLoRA
        "--defuse_gpt2_attn",  # å¯ç”¨defuse
        "--lora_r", "16",
        "--lora_alpha", "32",
        "--lora_dropout", "0.05",
        "--target_modules", "q_proj,k_proj,v_proj,attn.c_proj,mlp.c_fc,mlp.c_proj",
        "--per_device_train_batch_size", "2",  # å‡å°batch sizeä»¥é€‚åº”GPUå†…å­˜
        "--gradient_accumulation_steps", "8",
        "--max_steps", "50",  # å‡å°‘æ­¥æ•°è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        "--save_steps", "25",
        "--logging_steps", "5",
        "--learning_rate", "5e-5",
        "--warmup_steps", "5",
        "--block_size", "512",
        "--overwrite_output_dir",
        "--do_train",
    ]
    
    print("\nå¼€å§‹GPU MoSLoRAè®­ç»ƒæµ‹è¯•...")
    print(f"å‘½ä»¤: CUDA_VISIBLE_DEVICES=0 {' '.join(cmd)}")
    
    try:
        # è¿è¡Œè®­ç»ƒï¼Œä½¿ç”¨CUDA_VISIBLE_DEVICES=0
        full_cmd = f"CUDA_VISIBLE_DEVICES=0 {' '.join(cmd)}"
        result = subprocess.run(full_cmd, capture_output=True, text=True, timeout=300, shell=True)  # 5åˆ†é’Ÿè¶…æ—¶
        
        print("è®­ç»ƒå®Œæˆ!")
        print(f"è¿”å›ç : {result.returncode}")
        
        if result.stdout:
            print("æ ‡å‡†è¾“å‡º (æœ€å1000å­—ç¬¦):")
            print(result.stdout[-1000:])
        
        if result.stderr:
            print("é”™è¯¯è¾“å‡º (æœ€å1000å­—ç¬¦):")
            print(result.stderr[-1000:])
        
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print("è®­ç»ƒè¶…æ—¶!")
        return False
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False

def test_peft_loading():
    """æµ‹è¯•PEFTé€‚é…å™¨åŠ è½½"""
    
    output_dir = "/home/hangshuaihe/tlm/gen/test_gpu_moslora_output"
    base_model_path = "/home/hangshuaihe/tlm/tlm_dataset/gen/gen_data/clm_gen_best_v100_v4"
    
    print(f"\næµ‹è¯•PEFTé€‚é…å™¨åŠ è½½...")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        print(f"âœ— è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return False
    
    # æ£€æŸ¥PEFTæ–‡ä»¶
    adapter_config = os.path.join(output_dir, "adapter_config.json")
    adapter_model = os.path.join(output_dir, "adapter_model.bin")
    
    if not os.path.exists(adapter_config):
        print(f"âœ— æœªæ‰¾åˆ° adapter_config.json")
        return False
    
    if not os.path.exists(adapter_model):
        print(f"âœ— æœªæ‰¾åˆ° adapter_model.bin")
        return False
    
    print(f"âœ“ æ‰¾åˆ°PEFTé€‚é…å™¨æ–‡ä»¶")
    
    try:
        # æµ‹è¯•åŠ è½½
        print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
        base_model = AutoModelForCausalLM.from_pretrained(base_model_path)
        
        print(f"åŠ è½½PEFTé€‚é…å™¨: {output_dir}")
        model = PeftModel.from_pretrained(base_model, output_dir)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
        if torch.cuda.is_available():
            model = model.to("cuda:0")
            print(f"âœ“ æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {model.device}")
        
        # ç®€å•çš„æ¨ç†æµ‹è¯•
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.pad_token = tokenizer.eos_token
        
        test_input = "Hello, this is a test"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda:0") for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False)
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"âœ“ æ¨ç†æµ‹è¯•æˆåŠŸ: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âœ— PEFTåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gen_state_integration():
    """æµ‹è¯•ä¸gen_state.pyçš„é›†æˆ"""
    
    output_dir = "/home/hangshuaihe/tlm/gen/test_gpu_moslora_output"
    base_model_path = "/home/hangshuaihe/tlm/tlm_dataset/gen/gen_data/clm_gen_best_v100_v4"
    
    print(f"\næµ‹è¯•gen_state.pyé›†æˆ...")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬
    test_script = """
import os
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM

# æ·»åŠ PEFTè·¯å¾„
_BASE_DIR = os.path.dirname(os.path.abspath(__file__))
_LOCAL_PEFT_PATH = os.path.join(_BASE_DIR, "MosLora", "peft", "src")
if os.path.isdir(_LOCAL_PEFT_PATH) and _LOCAL_PEFT_PATH not in sys.path:
    sys.path.append(_LOCAL_PEFT_PATH)

from peft import PeftModel

# æµ‹è¯•å‚æ•°
model_name_or_path = "{adapter_dir}"
base_model_path = "{base_model_path}"

# æ£€æŸ¥æ˜¯å¦ä¸ºPEFTé€‚é…å™¨
adapter_config_path = os.path.join(model_name_or_path, "adapter_config.json")
if os.path.exists(adapter_config_path):
    print(f"Loading PEFT adapter from {{model_name_or_path}}")
    print(f"Using base model from {{base_model_path}}")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_path)
    model = PeftModel.from_pretrained(base_model, model_name_or_path)
    print("âœ“ PEFTé€‚é…å™¨åŠ è½½æˆåŠŸ!")
else:
    print(f"Loading standard model from {{model_name_or_path}}")
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
    print("âœ“ æ ‡å‡†æ¨¡å‹åŠ è½½æˆåŠŸ!")

print("é›†æˆæµ‹è¯•å®Œæˆ!")
""".format(adapter_dir=output_dir, base_model_path=base_model_path)
    
    # ä¿å­˜æµ‹è¯•è„šæœ¬
    test_file = "test_gen_state_integration.py"
    with open(test_file, "w") as f:
        f.write(test_script)
    
    try:
        # è¿è¡Œæµ‹è¯•
        result = subprocess.run(["python", test_file], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ“ gen_state.pyé›†æˆæµ‹è¯•æˆåŠŸ!")
            print(result.stdout)
        else:
            print("âœ— gen_state.pyé›†æˆæµ‹è¯•å¤±è´¥!")
            print(result.stderr)
        
        # æ¸…ç†
        os.remove(test_file)
        return result.returncode == 0
        
    except Exception as e:
        print(f"âœ— é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        if os.path.exists(test_file):
            os.remove(test_file)
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print("=== å®Œæ•´MoSLoRAæµ‹è¯• ===")
    
    # 1. GPUè®­ç»ƒæµ‹è¯•
    print("\n1. GPUè®­ç»ƒæµ‹è¯•")
    training_success = test_gpu_training()
    
    # 2. PEFTåŠ è½½æµ‹è¯•
    print("\n2. PEFTé€‚é…å™¨åŠ è½½æµ‹è¯•")
    loading_success = test_peft_loading()
    
    # 3. gen_state.pyé›†æˆæµ‹è¯•
    print("\n3. gen_state.pyé›†æˆæµ‹è¯•")
    integration_success = test_gen_state_integration()
    
    # æ€»ç»“
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"GPUè®­ç»ƒ: {'âœ“ æˆåŠŸ' if training_success else 'âœ— å¤±è´¥'}")
    print(f"PEFTåŠ è½½: {'âœ“ æˆåŠŸ' if loading_success else 'âœ— å¤±è´¥'}")
    print(f"é›†æˆæµ‹è¯•: {'âœ“ æˆåŠŸ' if integration_success else 'âœ— å¤±è´¥'}")
    
    if all([training_success, loading_success, integration_success]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! MoSLoRAç³»ç»Ÿå·¥ä½œæ­£å¸¸!")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()
